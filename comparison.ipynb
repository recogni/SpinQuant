{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of Quantization to Text Generation\n",
    "\n",
    "This notebook is designed to compare the responses of the **Llama 3.1 8B Instruct** model across different quantization configurations using a set of questions selected from the ArenaHard dataset. Specifically, we will examine and contrast the model's performance under the following quantization settings:\n",
    "\n",
    "- 16-bit (BF16)\n",
    "- 8-bit (W8A8KV8)\n",
    "- 4-bit (W4A4KV4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. To be able to follow the notebook, please first go through the installation steps in the README.md."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from transformers import LlamaTokenizerFast\n",
    "import transformers\n",
    "from eval_utils.modeling_llama import LlamaForCausalLM\n",
    "from eval_utils.main import ptq_model\n",
    "from utils import utils\n",
    "from utils.process_args import process_args_ptq\n",
    "\n",
    "log: Logger = utils.get_logger(\"spinquant\")\n",
    "MODEL_ID: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "HF_TOKEN: str = \"<YOUR HF_TOKEN>\" # Replace with your huggingface token\n",
    "MAX_NEW_TOKENS: int = 2000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1. Define questions from [ArenaHard](https://huggingface.co/spaces/lmarena-ai/arena-hard-browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts: list[str] = [\n",
    "    \"I have a dataset which contains a list of 2D images, given a new image, how to find the closest image in the dataset\",\n",
    "    \"I have black and white images with 1 pixel width white horizonal lines going through the image. How to detect the lines and remove it?\",\n",
    "    \"Given a binary array ‘nums’, you are required to find the maximum length of a contiguous subarray that contains an equal number of 0s and 1s.\",\n",
    "    \"Proof that Q(sqrt(-11)) is a principal ideal domain\",\n",
    "    \"Suppose you an architect of ad network platform that have a task to build a system for optimization of landing page (financial offers, like selling debit cards and getting comissions from it). You have a traffic flow (TF), conversions (CV), pay per click rates (CZ) or pay per offers (PA). Give outline and a concept code for such a system maximizing revenue. Apply thomson samling method (or similar optimal) to get fastest and accurate results from AB testing.\",\n",
    "    \"Can you market size revenue that can earned by UK Management Consultancy by advising and implementing FinTech solutions to Capital Markets clients\",\n",
    "    \"Show me how to make 1$ using 19 coins\",\n",
    "    \"How DO i perform continuous delta hedging with a neural network in python\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define a function to quantize Llama 3.1 8B Instruct\n",
    "This function will be used to get the quantized 8-bit and 4-bit versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(args):\n",
    "    sys.argv = args\n",
    "\n",
    "    model_args, training_args, ptq_args = process_args_ptq()\n",
    "    print(model_args)\n",
    "    print(training_args)\n",
    "    print(ptq_args)\n",
    "\n",
    "    config = transformers.AutoConfig.from_pretrained(\n",
    "        model_args.input_model, token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "    process_word_embeddings = False\n",
    "    if config.tie_word_embeddings:\n",
    "        config.tie_word_embeddings = False\n",
    "        process_word_embeddings = True\n",
    "\n",
    "    dtype = torch.bfloat16 if training_args.bf16 else torch.float16\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_args.input_model,\n",
    "        config=config,\n",
    "        torch_dtype=dtype,\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    if process_word_embeddings:\n",
    "        model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "    model.cuda()\n",
    "\n",
    "    model = ptq_model(ptq_args, model, model_args)\n",
    "    model.seqlen = training_args.model_max_length\n",
    "\n",
    "    log.info(\"Model PTQ completed {}\".format(model))\n",
    "    log.info(\"Start to load tokenizer...\")\n",
    "\n",
    "    tokenizer = LlamaTokenizerFast.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_args.input_model,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        add_eos_token=False,\n",
    "        add_bos_token=False,\n",
    "        token=model_args.access_token,\n",
    "    )\n",
    "    log.info(\"Complete tokenizer loading...\")\n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Implement a function to generate a response for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer):\n",
    "    responses = []\n",
    "    for prompt in prompts:\n",
    "        if tokenizer.chat_template is None:\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device).input_ids\n",
    "        else:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "        streamer = transformers.TextStreamer(tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        print(f\"Question:\\n{prompt}\\n\")\n",
    "        print(\"Generated answer from quantized model:\\n\")\n",
    "        start_generation_time = time.time()\n",
    "\n",
    "        output = model.generate(input_ids, max_new_tokens=MAX_NEW_TOKENS, use_cache=True, streamer=streamer, do_sample=False)\n",
    "        responses.append(output)\n",
    "        generation_time = time.time() - start_generation_time\n",
    "        print(f\"\\nTokens generated: {output.size(1)}.\")\n",
    "        print(f\"Time taken to generate the response: {generation_time:.2f} seconds.\")\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load 16-bit version\n",
    "\n",
    "Note: Loading all 3 different models for each setting at once might not be possible due to VRAM limitations, in that case load each model and generate responses seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 16-bit version:\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_ID, token=HF_TOKEN\n",
    ")\n",
    "\n",
    "llama_16bit_model = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=HF_TOKEN\n",
    ").to(device)\n",
    "\n",
    "llama_16bit_tokenizer = LlamaTokenizerFast.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    model_max_length=2048,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    add_eos_token=False,\n",
    "    add_bos_token=False,\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_16bit = generate_responses(llama_16bit_model, llama_16bit_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the 16-bit model and quantize it to lower-bit variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Quantize it to 8-bit and generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_8bit = [\n",
    "    \"2_eval_ptq.sh\",\n",
    "    \"--input_model\", MODEL_ID,\n",
    "    \"--do_train\", \"False\",\n",
    "    \"--do_eval\", \"True\",\n",
    "    \"--per_device_eval_batch_size\", \"4\",\n",
    "    \"--model_max_length\", \"2048\",\n",
    "    \"--fp16\", \"False\",\n",
    "    \"--bf16\", \"True\",\n",
    "    \"--save_safetensors\", \"False\",\n",
    "    \"--w_bits\", \"8\",\n",
    "    \"--a_bits\", \"8\",\n",
    "    \"--k_bits\", \"8\",\n",
    "    \"--v_bits\", \"8\",\n",
    "    \"--w_clip\",\n",
    "    \"--a_asym\",\n",
    "    \"--k_asym\",\n",
    "    \"--v_asym\",\n",
    "    \"--k_groupsize\", \"128\",\n",
    "    \"--v_groupsize\", \"128\",\n",
    "    \"--rotate\",\n",
    "    \"--optimized_rotation_path\", \"rotation_matrices/llama-3.1-8b-instruct/8-bit/R.bin\",\n",
    "]\n",
    "\n",
    "llama_8bit_model, llama_8bit_tokenizer = quantize(args_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_8bit = generate_responses(llama_8bit_model, llama_8bit_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Quantize it to 4-bit and generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_4bit = [\n",
    "    \"2_eval_ptq.sh\",\n",
    "    \"--input_model\", MODEL_ID,\n",
    "    \"--do_train\", \"False\",\n",
    "    \"--do_eval\", \"True\",\n",
    "    \"--per_device_eval_batch_size\", \"4\",\n",
    "    \"--model_max_length\", \"2048\",\n",
    "    \"--fp16\", \"False\",\n",
    "    \"--bf16\", \"True\",\n",
    "    \"--save_safetensors\", \"False\",\n",
    "    \"--w_bits\", \"4\",\n",
    "    \"--a_bits\", \"4\",\n",
    "    \"--k_bits\", \"4\",\n",
    "    \"--v_bits\", \"4\",\n",
    "    \"--w_clip\",\n",
    "    \"--a_asym\",\n",
    "    \"--k_asym\",\n",
    "    \"--v_asym\",\n",
    "    \"--k_groupsize\", \"128\",\n",
    "    \"--v_groupsize\", \"128\",\n",
    "    \"--rotate\",\n",
    "    \"--optimized_rotation_path\", \"rotation_matrices/llama-3.1-8b-instruct/4-bit/R.bin\",\n",
    "]\n",
    "\n",
    "llama_4bit_model, llama_4bit_tokenizer = quantize(args_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_4bit = generate_responses(llama_4bit_model, llama_4bit_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
